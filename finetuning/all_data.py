# æŠ¥å‘Š-æ”»å‡»è¿‡ç¨‹çš„æ‰€æœ‰ç»„åˆ ç”Ÿæˆ2000æ¡
import random
import json
import os
from collections import defaultdict
import spacy
import pandas as pd
import ast
import string
# conda activate svo_env

def extract_svo_triples(text):
    doc = nlp(text)
    triples = []

    for sent in doc.sents:
        for token in sent:
            if token.dep_.endswith("subj"):
                subject = token
                verb = token.head
                for child in verb.children:
                    if child.dep_.endswith("obj"):
                        triples.append((subject.text, verb.text, child.text))
    return triples

data_dir = "/Users/xinguohua/Code/pythonProject1/finetuning/data"
json_data_list = []
samples = []

######################################################################  æŠ¥å‘Š-æ”»å‡»æŠ€æœ¯ ###################################################################### ###################################
for root, dirs, files in os.walk(data_dir):
    for filename in files:
        if filename.endswith(".json"):
            file_path = os.path.join(root, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    json_data = json.load(f)
                    full_context = []
                    techniques = []
                    for item in json_data:
                        ctx = item.get("context", "")
                        tech = item.get("technique", "")
                        if ctx and isinstance(ctx, str) and ctx.strip():
                            full_context.append(ctx.strip())
                        if tech and isinstance(tech, str) and tech.strip():
                            techniques.append(tech.strip())
                    if full_context and techniques:
                        samples.append({
                            "context": " ".join(full_context),
                            "techniques": techniques,
                            "source_file": file_path
                        })
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    json_data_list.append(data)
            except Exception as e:
                print(f"è¯»å–å¤±è´¥ï¼š{file_path}ï¼Œé”™è¯¯ï¼š{e}")
print(f"\n æ ·æœ¬ {samples}")

######################################################################  æ”»å‡»æŠ€æœ¯-è¿‡ç¨‹ ###################################################################### ###################################

base_dir = "/Users/xinguohua/Code/pythonProject1/finetuning/mitre-ttp-mapping"
dataframes = {}
all_rows = []

for root, _, files in os.walk(base_dir):
    for file in files:
        if file.endswith(".tsv"):
            file_path = os.path.join(root, file)
            try:
                df = pd.read_csv(file_path, sep="\t")
                df["source_file"] = file  # æ·»åŠ æ¥æºåˆ—
                all_rows.append(df)
                print(f"âœ… è¯»å– {file}: {df.shape[0]} è¡Œ")
            except Exception as e:
                print(f"âŒ è¯»å–å¤±è´¥: {file_path}ï¼Œé”™è¯¯: {e}")

# åˆå¹¶æ‰€æœ‰æ•°æ®
df_all = pd.concat(all_rows, ignore_index=True)
print(f"df_all{df_all}")
label2texts = defaultdict(list)
label2triples = defaultdict(list)
nlp = spacy.load("en_core_web_sm")

for _, row in df_all.iterrows():
    try:
        labels = ast.literal_eval(row["labels"])
        if isinstance(labels, list) and len(labels) == 1:  # åªå¤„ç†ä¸€ä¸ªæ ‡ç­¾çš„æƒ…å†µ
            label = labels[0]
            label2texts[label].append(row["text1"])
            text = row["text1"]
            triples = extract_svo_triples(text)
            label2triples[label].append(triples)
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {row['labels']}ï¼Œé”™è¯¯: {e}")

# å»ºç«‹ æ”»å‡»æŠ€æœ¯ -> è¿‡ç¨‹ä¸‰å…ƒç»„ çš„æ˜ å°„
print(f"ğŸ¯ æ˜ å°„ç¤ºä¾‹: {list(label2texts.items())[:5]}")
print(f"ğŸ¯ æ˜ å°„ç¤ºä¾‹: {list(label2triples.items())[:5]}")


combined_samples = []

for sample in samples:
    context = sample["context"]
    techniques = set(sample["techniques"])
    all_process_triples = []
    for tech in techniques:
        if tech in label2triples:
            triples_group_list = label2triples[tech]
            valid_triples = [t for t in triples_group_list if t]
            if valid_triples:
                selected_triples = random.choice(valid_triples)  # âœ… éšæœºé€‰ä¸€ä¸ª
                all_process_triples.append(selected_triples)
    combined_samples.append({
        "report": context,
        "technique": tech,
        "triples": all_process_triples
    })

print(f"combined_samples{combined_samples}")

# # æ„å»ºé€‰æ‹©é¢˜
# {
#   "instruction": "Select the candidate tuple(s) below that best match the described attack behavior. Multiple or no selections are allowed. Each tuple is in the format (Entity_1, Relation, Entity_2).",
#   "input": "Report: {The attack report in motivate.pdf}\nOptions: {The corresponding candidate tuple in reason.json}",
#   "output": "Options"
# }
#
# # æ„å»ºåˆ¤æ–­é¢˜
# {
#   "instruction": "Determine whether the described attack paths fully support the attack report. Each path consists of multiple entity-relation elements in the format (Entity_1, Relation_1, Entity_2, Relation_2, Entity_3).",
#   "input": "Report: {The attack report in motivate.pdf}\nAttack Path: {The corresponding path in reason.json}",
#   "output": "Yes"
# }

multiple_choice_questions = []
true_false_questions = []


for sample in combined_samples:
    report = sample["report"]
    triple_groups = sample["triples"]
    techniques = sample["technique"]
    flat_triples = [t for group in triple_groups for t in group]

    non_related_triples = []
    for label, triples_list in label2triples.items():
        if label not in techniques:
            for group in triples_list:
                if group:
                    non_related_triples.extend(group)

    if not flat_triples:
        continue
     #  å¤šç”Ÿæˆå‡ ç»„é¢˜ï¼ˆæ¯”å¦‚æ¯ä¸ª sample ç”Ÿæˆ 3ï½5 é“é¢˜ï¼‰
    repeat_count = random.randint(2, 4)
    for _ in range(repeat_count):
    # å°†çœŸå®ä¸‰å…ƒç»„æ‰“ä¹±ï¼Œç¡®ä¿ä¸ä¼šé›†ä¸­å‡ºç°åœ¨åŒä¸€é¢˜ä¸­
        formatted_triples = ["({}, {}, {})".format(*t) for t in flat_triples]
        random.shuffle(formatted_triples)

        # åˆ†æ‰¹æ„å»ºé¢˜ç›®ï¼Œæ¯é¢˜åŒ…å« 1~2 ä¸ªçœŸå®ä¸‰å…ƒç»„
        i = 0
        while i < len(formatted_triples):
            num_correct = min(random.randint(1, 2), len(formatted_triples) - i)
            selected_correct = formatted_triples[i:i + num_correct]
            i += num_correct

            # é€‰ 2~3 ä¸ªå‡ä¸‰å…ƒç»„
            fake_count = random.randint(2, 3)
            fake_samples = random.sample(non_related_triples, k=fake_count)
            fake_options = ["({}, {}, {})".format(*t) for t in fake_samples]

            # ç»„åˆå¹¶æ‰“ä¹±é€‰é¡¹
            all_options = selected_correct + fake_options
            random.shuffle(all_options)

            # ç¼–å· A. B. C. ...
            lettered_options = []
            correct_answers = []
            for idx, opt in enumerate(all_options):
                letter = string.ascii_uppercase[idx]
                option_str = f"{letter}. {opt}"
                lettered_options.append(option_str)
                if opt in selected_correct:
                    correct_answers.append(letter)

            multiple_choice_questions.append({
                "instruction": "Select the candidate tuple(s) below that best match the described attack behavior. Multiple or no selections are allowed. Each tuple is in the format (Entity_1, Relation, Entity_2).",
                "input": f"Report: {report}\nOptions:\n" + "\n".join(lettered_options),
                "output": ",".join(correct_answers)
            })

    # 2ã€æ„å»ºåˆ¤æ–­é¢˜ï¼ˆæ„é€ æ”»å‡»è·¯å¾„ï¼‰
    #  æ„å»ºå®Œæ•´è·¯å¾„ï¼ˆYesï¼‰
    full_path = []
    for group in triple_groups:
        for t in group:
            full_path.extend(t)

    if len(full_path) < 6:
        continue  # è‡³å°‘ä¸¤ä¸ªä¸‰å…ƒç»„ï¼Œè·³è¿‡è¿‡çŸ­è·¯å¾„

    true_false_questions.append({
        "instruction": "Determine whether the described attack paths fully support the attack report. Each path consists of multiple entity-relation elements in the format (Entity_1, Relation_1, Entity_2...).",
        "input": f"Report: {report}\nAttack Path: {tuple(full_path)}",
        "output": "Yes"
    })

    #  æ„å»ºå„ç§å­è·¯å¾„ï¼ˆNoï¼‰
    n = len(triple_groups)
    for i in range(1, n):  # ä»1å¼€å§‹ï¼Œè·³è¿‡å®Œæ•´é•¿åº¦
        # a -> b
        partial = triple_groups[:i]
        partial_path = []
        for group in partial:
            for t in group:
                partial_path.extend(t)
        true_false_questions.append({
            "instruction": "Determine whether the described attack paths fully support the attack report. Each path consists of multiple entity-relation elements in the format (Entity_1, Relation_1, Entity_2...).",
            "input": f"Report: {report}\nAttack Path: {tuple(partial_path)}",
            "output": "No"
        })

    # å¯é€‰ï¼šaã€bã€c å•ç‹¬ä¹Ÿå¯ä»¥å½“ Noï¼ŒåŠ ä¸Šè¿™éƒ¨åˆ†ä¹Ÿè¡Œ
    for group in triple_groups:
        if len(group) < 2:
            continue
        partial_path = []
        for t in group:
            partial_path.extend(t)
        true_false_questions.append({
            "instruction": "Determine whether the described attack paths fully support the attack report. Each path consists of multiple entity-relation elements in the format (Entity_1, Relation_1, Entity_2...).",
            "input": f"Report: {report}\nAttack Path: {tuple(partial_path)}",
            "output": "No"
        })


# å…± 2788 æ¡
mc_path = "multiple_choice_questions.json"
with open(mc_path, "w", encoding="utf-8") as f:
    json.dump(multiple_choice_questions, f, ensure_ascii=False, indent=2)
print(f" Multiple-choice questions saved to {mc_path}ï¼Œå…± {len(multiple_choice_questions)} æ¡")

# å…± 1127 æ¡
# ä¿å­˜ true/false é¢˜ç›®
tf_path = "true_false_questions.json"
with open(tf_path, "w", encoding="utf-8") as f:
    json.dump(true_false_questions, f, ensure_ascii=False, indent=2)
print(f" True/False questions saved to {tf_path}ï¼Œå…± {len(true_false_questions)} æ¡")